
\chapter[Choosing transcriptomics analyses]{Choosing reliable, reproducible 
           transcriptomics analyses}\label{chapter:intro3}

Throughout this dissertation, I present analyses of the expression of the genomes
of human cells or tissues of mice that have been treated 
with \textit{C. difficile} toxins A and B
(TcdA and TcdB) in order to determine the pathogenic responses
of the host at the cellular level. 
In learning how to process this type of data,
I encountered many limitations and misunderstandings in 
common gene expression analyses. In this chapter, I describe
the background of this type of data and data analyses that form 
the base of this dissertation, and explain some important considerations
for interpreting sometimes very different results 
produced by different data analyses, an overlooked problem in the
majority of gene expression studies.

\section{Background}

\subsection{mRNA as a measure of cell state}

Our genomes are a sequence of $\sim$3-billion ``letters'' from a four-letter
alphabet of nucleobase molecules (\textit{bases}) \cite{Lander:2001wi}. 
Each of our $\sim$20,000 genes is pieced together from, on average,
5 physically separate sequences called \textit{exons}. Exons, which
range from $\sim$20 to 1,600 bases, are contained
within one of the 46 strands of DNA in our cells \cite{Michael:1999dm}.

The central dogma of molecular biology is 
DNA $\rightarrow$ RNA $\rightarrow$ protein \cite{Crick:1970wb}.
In each cell, exons are
copied (\textit{transcribed}) and spliced into portable
messenger RNA (mRNA). 
mRNA is then \textit{translated} to strings
of amino acids that arrange themselves into shapes with 
chemical properties that perform specific tasks.
These amino acid strings, or proteins, are the primary functional 
units that execute the instructions in our DNA.

When a protein is needed, a cell's ``circuitry'' 
triggers a gene (the DNA encoding that protein)
to be transcribed to mRNA that is then translated to the protein.
Specific amounts of proteins are produced
in response to different stimuli. Since the proteins will alter
the physical state of cells and consequently the body's overall physiological responses, 
many scientists have striven to understand the arrangement and logic
of this regulatory circuitry.

Most of this circuit's components were identified soon after the
sequencing of the human genome in 2003 \cite{Consortium:2004bm,Gregory:2006tu}.
This and decades of previous biological research provided a very basic view
of connections within the cell, yet many of the studies delineating
functions were limited to small sets of genes and proteins.
To be able to understand how all the components affect each other,
there had to be a way to take snapshots of the levels of thousands more 
mRNAs or proteins.

In 1982, technology to simultaneously measure
genome-wide gene expression (i.e., the levels of mRNA in a cell)
from a collection of cells was already being developed \cite{Augenlicht:1982wo}. 
Current RNA sequencing technologies
can now count individual mRNA molecules (\textit{transcripts}) from a collection of cells
for \${}1000 or less.

\subsection{Measuring mRNA levels with microarrays}

DNA molecules consist of two, connected, parallel strands, each strand 
containing a sequence of nucleobases along a sugar backbone.
The four bases (adenosine (A), thymine (T), cytosine (C), guanine (G))
join to each other by hydrogen bonds. A only pairs with T; C only pairs with G.
Two strands \textit{hybridize} when their base pairs are aligned.

Strand-specific hybridization can be used to identify
DNA sequences from uncharacterized samples. For example, single-stranded DNA
with a known sequence
can be fixed to a substrate or surface, and DNA from an unknown
source can then be labeled and washed over that surface.
If DNA from the two sources have matching strands (i.e., they
\textit{complement} one another), the labeled
DNA will hybridize and be detected.
The first ``gene arrays'' that could detect multiple sequences in this way
were built by attaching 
DNA to hundreds of spots (\textit{probes}) on glass plates or 
slides \cite{Maskos:1992co, Augenlicht:1982wo}.
Each probe contained thousands or millions of DNA 
molecules with the same sequence so that one gene was detected per probe.

Since hybridization requires two DNA samples, mRNA must
be reverse transcribed back to DNA if it is to be measured on a gene array.
The resulting complementary DNA (cDNA)
can then be labeled and detected.
Signal intensities from the probes indicate the relative amounts of each
mRNA in a sample.


Microarrays, very small gene arrays, were introduced in 1995 \cite{Schena:1995fy}. 
Although microarrays require more sophisticated
manufacturing, they are based on the principles of older gene arrays.


\section[Microarray preprocessing]{Microarray preprocessing techniques}

The most commonly used microarrays over the past decade
have been made by Affymetrix. I described here many methods
designed for these arrays, yet the principles can be extended to
most other microarray technologies and even sequencing data.

\subsection{Steps of data preprocessing}\label{introjtm:steps}

\subsubsection{Affymetrix microarrays}

In less than one square inch, Affymetrix arrays fit over one million probes, enough
to measure genome-wide expression (the \textit{transcriptome}).
Since exons are longer than the 25-nucleotide probes,
\textit{probe sets} of ten to fifteen probes are designed to hybridize
one gene or exon.

\subsubsection{Nonspecific hybridization}

For each probe sequence, Affymetrix made a
\textit{mismatch} probe with the 13\textsuperscript{th} 
base changed. 
By subtracting the mismatch signal from the \textit{perfect-match} signal,
\textit{nonspecific hybridization} (\textit{cross-hybrdization} 
from other transcripts) may be 
estimated (\textit{perfect-match correction}).
However, mismatch hybridization is complex.
Usually, more than one third of mismatch probes have a higher
signal than their perfect match probes \cite{Irizarry:2003ge}. 
This should theorectically never happen. Several have proposed
hierarchical models with nonlinear terms that
better account for mismatch probes 
\cite{Li:2001jv,Milo:2003tt,Liu:2005ey,Hein:2005ip}, yet algorithms
that ignore mismatch probes perform equally
well or better \cite{Chen:2007cr,Irizarry:2003ge,Hochreiter:2006ja}.

\subsubsection{Background correction}

Since mismatch probes cannot be used and there is no 
empty space on high-density arrays, 
background signals must be estimated from many
probes. Affymetrix first proposed splitting an array
into zones and calculating background signals from low
intensity probes. This approach systematically corrects large
sections of the array,
yet it does not address probe-specific background signals.

Irizarry et al. observed the distribution
of all observed probe signals ($O$) could be approximated by
a mixture of an exponential distribution ($S$) and a
normal distribution ($B$) \cite{Irizarry:2003ge}.
$S$ and $B$ were considered the true signal and background signal, respectively ($O=S+B$).
After the mean and variance of $S$ and $B$ are estimated from the data, 
the background-corrected signal can be calculated 
as $\text{E}[S|O=o]$ by the robust multi-array average (RMA) procedure in
\cite{Irizarry:2003ge}. 
Wu et al. introcuded gcRMA, which improved upon RMA by 
accounting for sequence-specific probe
affinities that were determined from previous experiments
($O=S+B+N$ where $N$ is non-specific hybridization due to
differences in probe sequences) \cite{Wu:2004wh}. gcRMA
also modeled mismatch probes by making two equations, one for 
$O_{\text{mismatch}}$ and one for $O_{\text{perfect}}$, with one common 
term, the true signal $S$. 
Similarly, other model-based expression value calculations
have the option to include or ignore
mismatch probes (e.g.,\cite{Li:2001jv,Milo:2003tt}).

\subsubsection{Probe set summarization}

To estimate a gene's expression, the probes in a probe
set must be summarized. 
Since outliers are common, robust
statistics (e.g., median) are preferred.
Affymetrix first recommended the Tukey bi-weight
statistic, calculating probe set values one microarray at a time.
However, many probes have similar effects across all microarrays
(e.g., different affinities), and these \textit{probe effects} can 
be modeled and removed as was shown by Li and Wong 
(\cite{Li:2001jv,Li:2001wk} is often called the ``Li-Wong method'').
The most popular summarization, ``median polish'', places a probe set's expression
values $a_{ij}$ in a matrix ($i$ indicates the probe an $j$ 
indicates the array). The row and column medians are 
iteratively subtracted to estimate an error matrix. Probe effects and
expression values are calculated from the sum of the subtracted row 
and column medians, respectively, minus the 
medians of both vector sums \cite{Mosteller:1977vp,Irizarry:2003ge}.  
Chen et al.'s ``distribution-free, weighted'' summarization
accounts for probe effects primarily by assuming that low-variability
probes are high-quality and should be weighted more 
than other probes \cite{Chen:2007cr}.
Hochreiter et al. assume perfect match probes are normally distributed, enabling
them to perform `factor analysis' 
where the `factors'
are mRNA concentrations \cite{Hochreiter:2006ja}. 
Even more complex models may incorporate 
background correction , perfect-match correction, and probe set summarization
into one step (e.g. \cite{Zhang:2003to,Liu:2005ey}), 
yet each step is typically performed separately.

\subsubsection{Normalization}

Systematic differences between arrays must be normalized if they
are to be compared. The simplest normalization procedures center
all array values by mean, median, or some other measure, yet centering
doesn't account for different ranges of values.
Quantile normalization forces two arrays to have the exact same
statistical distribution \cite{Bolstad:2003ia}. Li and Wong's normalization
iteratively searches for a group of ``householding genes'' (the invariant 
set of probes) that will be forced to the same expression values in all arrays,
and all other probes are adjusted accordingly \cite{Li:2001wk}.
Huber et al. found that the inverse hypberbolic sine transformation
made probe variance less dependent on probe mean \cite{Huber:2003bw}.
This variance stabilization---in combination with a nonlinear model fit to find
scaling factors and offsets for each microarray---is used for normalization.
Loess normalization applies a smoother to an `MA plot' which plots
the differences between two arrays ($M=log_2(x_1/x_2)=log_2(x_1)-log_2(x_2)$)
versus the average signal of two arrays ($A=\frac{1}{2}log_2(x_1x_2)=\frac{1}{2}(log_2(x_1)+log_2(x_2))$).
The smoother is subtracted from each point so that the plot
is centered around the $A$-axis. Loess normalization thus makes
offset adjustments that are dependent on the intensity of the signal.


\section{Choosing preprocessing techniques}

\subsection{What are the best preprocessing steps?}\label{introjtm:whatsbest}

Usually, the answer is ``we're not sure'' or ``it depends''. 
Hundreds of methods have been published.
Which ones are chosen depends on if the method's assumptions match
the experimental design.

Selecting the full sequence of steps (the \textit{workflow}) is daunting.
The techniques in \ref{introjtm:steps} are a subset of the many choices.
For each step (background correction, normalization,
perfect-match correction, and probe set summarization), there are
ten or more possible algorithms making at least $10^4=10,000$ possible
workflows. However, each algorithm has at least one, sometimes
three or four arbitrarily or heuristically chosen parameters, making
for over ten million possible workflows (actually real number parameters make 
for infinite workflows).
Since there are no easy general guidelines, I present some
illustrative examples below. 

\subsubsection{Background correction and perfect-match correction}

Background correction algorithms essentially deconvolute the observed
signal into two signals: the noise and true signal.
They are most helpful then for low-signal probes where
the signal to noise ratio is lowest. If researchers are uninterested
in low-abundance transcripts, they might consider skipping
background correction.

Background correction is the least understood step because
``there is currently no way to design 
an oligonucleotide microarray such that the probes have 
fully predictable hybridization'' \cite{Pozhitkov:2007go}.
gcRMA and PDNN use sequence data to try and infer complex
probe affinities from sequence data yet are not much better than
some that do not \cite{Irizarry:2003ge,Zhang:2003to,Wu:2004wh}.
Affymetrix's MAS5.0 background correction and perfect-match corrections
are not model based, making simple assumptions on how low-intensity probes
or mismatch probes should adjust surrounding perfect-match probes.
Although there is no clear, universal support of one method over another,  
it is commonly accepted that 
mismatch signals should be ignored or modeled in some way as 
contributing to the observed signal.

\subsubsection{Normalization}

If one is sure of impeccable sample isolation and
reproducibility, they might decide against normalization. 
However, since even the slightest differences in one of many experimental factors 
(e.g., scanner reproducibility, mRNA concentratioin calculations, hybridization
temperatures) cause systematic errors, there should be strong 
justification for skipping normalization.

If researchers believe that only a few dozen
of thousands of transcripts vary between arrays, then the distribution
of expression values should be similar. Quantile
normalization would then be appropriate. If treatment
systematically increases the total mRNA per cell, quantile
normalization would incorrectly mean-center all arrays 
(though uncommon, cells may need to be counted before RNA isolation \cite{Loven:2012km}).
Loess normalization might be better since it 
modifies each array's values according to similarly expressed 
probes on other arrays, allowing for slightly different distributions.
If the primary problem is high variance of low-signal probes, variance
stabilization may be best. If $~$10 ``householding'' genes are known
or can be trusted to be found algorithmically, invariant set normalization
would work. However, since invariant set normalization assumes probe affinities
are similar, probe values must be corrected in background correction.
Treatment groups may be so different that
no normalization cannot be justified. The treatment groups might then
be separately normalized, although subsequent comparisons may be difficult to interpret.

\subsubsection{Probe set summarization}

There are no general rules for probe set summarization, yet
there are mistakes to be avoided. Outliers are common in microarrays
so robust measures of center are used.
It is recommended to choose summaries
that ``borrow'' information from all arrays to identify probe effects.
Though different summarization techniques may produce significantly
different expression values, no technicque is necessarily incorrect.
However, the artifacts introduced by some techniques may
cause incorrect interpretations in downstream analyses (see \ref{introjtm:difgoals}).

Summarization reduces 10+ probes to one value, so 90\% of the data
is lost. Therefore, before summarization, one might use
the distribution of probe values to perform more powerful statistical tests or to propogate
error through subsequent steps \cite{Milo:2003tt,Liu:2005ey}.
Nevertheless, probe set summarization must eventually be performed
at some level if one wants to study genes, not 25-nucleotide stretches of DNA.

\subsubsection{The order of steps}
There is no required order for each step of the analysis,
yet there are limitiations. For instance, if probe set summarization were done first,
probe values would not be available to estimate the background signal. 
Perfect-match correction also wouldn't
be possible. Normalization can be applied before and/or after summarization.
There is no strong evidence supporting one choice over the other.
It is also unclear if normalization should be done for all arrays at once
or separately for subsets (e.g., control and treatment groups).

With few exceptions, each preprocessing method is modular, compatible
with any other method. However, very few have explored
the effects effects of combining algorithms.
For example, it may be the case that a background correction
invalidates assumptions for some normalization procedures.

\subsubsection{Different choices for different goals}\label{introjtm:difgoals}

Different analyses work better for different problems.
For example, Zhang et al claimed their PDNN model 
was superior to dChip ("Li-Wong" method)
and MAS5.0 (Affymetrix default) \cite{Zhang:2003to}.
However, Wu and Irizzary commented that their
RMA and gcRMA methods performed as well or better for predicting
mRNA concentrations \cite{Wu:2004ul}.
Zhang et al. responded that their concentration predictions
are off by a predictable scale factor, and that
the ability to detect differentially expressed genes
was better than RMA and gcRMA \cite{Zhang:2004tl}. They were also unable to
reproduce results supporting Wu and Irizarry's claims.
It was unclear what the goal should be: accurate predictions of mRNA levels or
differentially expressed genes?

If the goal of a study is to compare the profiles of many genes
or samples, one must be aware of artifacts 
probe set summarization algorithms that severely
overestimate correlations.
Lim et al. observed, with gcRMA, an average correlation coefficient of 0.4
among randomly generated, uncorrelated arrays \cite{Lim:2007gc}. 
Since correlation measures are essential
for reverse engineering regulatory networks, previous network
studies that used gcRMA were flawed. 
Giorgi et al. identified that the median polish algorithm introduces 
high inter-sample correlations among randomly generated arrays \cite{Giorgi:2010jw}.
Their solution was to transpose the matrix of probe intensities
for each probe set, thereby transferring the error so that probes
would be overly correlated, not samples.
Therefore, if a study's goal is to classify patients by a clustering
algorithm, one should use the median polish algorithm very carefully.
If the goal is only to detect differentially expressed genes, then
the algorithm will not cause critical errors.


\subsubsection{Gold standards?}

To once and for all determine the best
preprocessing methods, Choe et al. spiked in
5,700 transcripts at various concentrations 
on 18 arrays (called ``Golden Spike'' \cite{Choe:2005dd})
By quantifying the accuracy of predicted differentially expressed genes, they defined
one best performing workflow, though several others 
performed similarly well. Several authors
noted flaws in the experimental design and analysis
causing unrealistic conclusions \cite{Pearson:2008jr,Schuster:2007wr,
Dabney:2006ud,Irizarry:2006bq,Gaile:2007ei,Fodor:2007wb}, and
a ``Platinum Spike'' data set was generated to address the flaws \cite{Zhu:2010jx}.
Conflicting recommendations from various authors that stemmed from
these data suggests 
that there will likely never be one ``best'' workflow, yet the analyses
made possible by these data sets
highlighted advantages and pitfalls each method (many of which
are discussed in \ref{introjtm:whatsbest}) and led to the invention of new techniques.

\subsection{Which workflow do I use?}

The previous sections have shown that I or anyone
else cannot definitively choose the right workflow. 
Instead, I would recommend an exploratory approach 
tailored for each data set.
For example, one could analyze there data set with ten 
or more different workflows and compare the results
using diagnostic tools (e.g., correlation matrices, clustering,
principal components analysis (PCA), and MA plots).
For example, one may find, as I did in one case, that invariant set
normalization causes outlier arrays (identified by PCA) because
of the automatically selected ``householding genes''.
Using an MA plot, they may then notice extraordinarily
high variance in the fold changes of low-signal probes, and then decide
on a workflow (e.g., mmGmos) that propagates this error to
statistical tests for differential expression. If expression levels
from arrays will be used as parameters in another model (e.g., a model
of metabolic flux where expression levels indicate the presence of 
different enzymes), then the high-variance, low-signal expression values
might all be set to a common threshold.
Interactive, easy-to-use diagnostic visualizations that allow for these
decisions are desperately lacking.
Although developing visualizations is a tremendous technical challenge,
there is a great oppurtunity for improvement in this area.

\section[Differential expression]{Detecting differentially expressed genes}

After microarray preprocessing, the next step is identify differentially
expressed genes (DEGs) between two treatment groups. Like preprocessing,
there are many methods and 

\subsubsection{Parametric tests}

\subsubsection{Permutation tests}

\subsubsection{Thoughts}

\section{Caveats of microarrays and alternatives}

Most of the problems with microarrays are low-intensity probes or
low-abundance transcripts. Discussion.

The second problem is that we don't understand how transcripts
interact with surfaces. These problems can hopefully be avoided
with RNA sequencing.

\section{Future improvements in transcriptomics analyses}

There are even more caveats to microarrays than the ones presented
above. Pozhitkov et al. review many of these limitations and go
so far as stating that ``there is currently no way to design 
an oligonucleotide microarray such that the probes have 
fully predictable hybridization'' \cite{Pozhitkov:2007go}.


The greatest need is visualization, Culhane et al provide a visualization
for data sets from different studies and platforms \cite{Culhane:2003it}.
Le Cao improved upon Culhane et al \cite{Cao:2009dd}.
Other methods have been made to correct for cross-platform differences \cite{Fan:2011iy}.

Ensemble methods
It is possible that data from different platforms could be combined
to provide additional information \cite{Hockley:2009tf}.
Models may also be combined as they were in predictive models
of cancer diagnostics \cite{Chen:2011ib}.

very important to have the correct probe annotations \cite{Carter:2005bq}.

\section{Recommendations}

Low-abundance transcripts are the biggest problem. Experimental methods should
specifically target this problem.

%http://camda.bioinfo.cipf.es/camda07/agenda/pdfs/submission_21.pdf
Most metrics will work for low and high intensity probes

Make clear the confidence in predictions from gene expression studies

\section{Visualizing results}

\section{Reproducibile analyses}

\subsection{Why bother?}

Irreproducible analyses are dangerous, perhaps bordering on unethical negligence. 
Dave et al. in the \textit{New England
Journal of Medicine} reported a marker for follicular lymphoma \cite{Dave:2004vl}. 
In letters to the editor, Tibshirani and Hong et al. stated they 
could not reproduce the analyses \cite{Tibshirani:2005el}. 
Dave et al. rectified the discrepancy as a misunderstanding
in how their data was interpreted. This is just one of a handful of
public disputes in the literature about gene expression analyses (two mentioned
in previous paragraphs).
Authors of disputed studies are most always well-intentioned, yet 
irreproducible analyses or poorly presented data raise suspicions.

In my opinion, analysts shouldn't fear being wrong; most researchers
are wrong most of the time. They must fear being overconfident or misleading. 
By making data and code open to criticism,
scientists protect their integrity and intellectual property in the same
way that lab notebooks do. They protect their colleagues
whose careers are dependent on their analyses as well as the patients
whose health decisions are affected by their research.
Finally, they increase there chance for mutually beneficial collaborations.

\subsection{Are microarrays reproducible?}

Thre has been great concern about inter-lab repeatability of 
gene expression experiments
(see example of poor reproducibility in 
\cite{Evsikov:2003ic,Fortunel:2003eu,Ivanova:2003bh,RamalhoSantos:2002dy}). 
Rather than discrediting microarray analysis,
one should also consider that micorarrays
may reveal differences in \textit{seemingly} same experimental protocols.
For example, with the data presented in this dissertation, arrays from
replicate experiments on different days were clearly different when
visualized with principal components analysis. However, the differences
were systematic and could be corrected.

Problems may also arise from overexpectations or and misunderstanding 
of what expression values can predict. 
For example, comparisons of microarray classification
studies (e.g., \cite{Veer:2002vv,Yeoh:2002wg,Alizadeh:2000tn,Golub:1999fn,
Perou:1999fr,Wang:2005uu,VanDeVijver:2002wj,Rosenwald:2002eg,Beer:2002uy,
Bhattacharjee:2001dt,Ramaswamy:2003to,Pomeroy:2002wx,Iizuka:2003wo})
with non-microarray studies have indicated predictive cancer markers or profiles
may not be as reliable as hoped \cite{Miklos:2004wn,Michiels:2007vs,Koscielny:2010gg,Eden:2004ua}.
Hundreds or even thousands of microarrays may be necessary to
accurately predict cancer 
outcomes \cite{Ioannidis:2005kd,Michiels:2005tg,EinDor:2006ga,Frantz:2005ur}.
However, a more rigorous re-evaluation, of a pessimistic study claiming
that microarray studies cannot predict cancer markers, found that
markers can indeed be found (see \cite{Michiels:2005tg,Fan:2010cv,Tong:2010ka,Koscielny:2010jd}
for the debate).
Hence, one shouldn't jump to conclusions from any one or even a few
analytical workflows.
For instance, two studies may find very different
list of differentially expressed genes, yet the correlation
between the data sets may be strong. Alternatively, two studies may
predict two different results that are both correct \cite{Zhang:2008bk,Zhang:2009cy}.

Several studies 
raise concerns about experimental reproducibility between
experimental platforms \cite{Tan:2003be,Kuo:2002cl,
Mah:2004ia,Rogojina:2003te,Woo:2004wz,Li:2002cz,Kothapalli:2002gz}.
Many irreproducibility claims were incorrect, first
dismissed by scientsist at the FDA \cite{Shi:2005ik}.
The FDA and EPA coordinated a Microarray Quality Control (MAQC) project
to set standards and resolve outstanding questions 
\cite{Lesko:2004vi,Frueh:2006uy,Dix:2006ty,Shi:2004vh}.
With careful quality assurance and analyses,
microarray data were similar between 
laboratories \cite{Dobbin:2005wj,Irizarry:2005kb,Larkin:2005hb,
Ulrich:2004vw,Waring:2004vh,Weis:2005ut} and
platforms \cite{Petersen:2005by,Yauk:2004fq,
Park:2004ux,Yuen:2002ha,Canales:2006ts,Shippy:2006uf,
Patterson:2006hi,Tong:2006cg,Guo:2006wi}.

\subsection{How to make reproducible analyses?}

\subsection{Reproducibility of this dissertation}

Being wrong as a scientist is expected, but being wrong in a way
that other


disput




