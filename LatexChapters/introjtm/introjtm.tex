
\chapter[Choosing transcriptomics analyses]{Choosing reliable, reproducible 
           transcriptomics analyses}\label{chapter:intro3}

Throughout this dissertation, I present analyses of the expression of the genomes
of human cells or tissues of mice that have been treated 
with \textit{C. difficile} toxins A and B
(TcdA and TcdB) in order to determine the pathogenic responses
of the host at the cellular level. 
In learning how to process this type of data,
I encountered many limitations and misunderstandings in 
common gene expression analyses. In this chapter, I describe
the background of this type of data and data analyses that form 
the base of this dissertation, and explain some important considerations
for interpreting sometimes very different results 
produced by different data analyses, an overlooked problem in the
majority of gene expression studies.

\section{Background}

\subsection{mRNA as a measure of cell state}

Our genomes are a sequence of $\sim$3-billion ``letters'' from a four-letter
alphabet of nucleobase molecules (\textit{bases}) \cite{Lander:2001wi}. 
Each of our $\sim$20,000 genes is pieced together from, on average,
5 physically separate sequences called \textit{exons}. Exons, which
range from $\sim$20 to 1,600 bases, are contained
within one of the 46 strands of DNA in our cells \cite{Michael:1999dm}.

The central dogma of molecular biology is 
DNA $\rightarrow$ RNA $\rightarrow$ protein \cite{Crick:1970wb}.
In each cell, exons are
copied (\textit{transcribed}) and spliced into portable
messenger RNA (mRNA). 
mRNA is then \textit{translated} to strings
of amino acids that arrange themselves into shapes with 
chemical properties that perform specific tasks.
These amino acid strings, or proteins, are the primary functional 
units that execute the instructions in our DNA.

When a protein is needed, a cell's ``circuitry'' 
triggers a gene (the DNA encoding that protein)
to be transcribed to mRNA that is then translated to the protein.
Specific amounts of proteins are produced
in response to different stimuli. Since the proteins will alter
the physical state of cells and consequently the body's overall physiological responses, 
many scientists have striven to understand the arrangement and logic
of this regulatory circuitry.

Most of this circuit's components were identified soon after the
sequencing of the human genome in 2003 \cite{Consortium:2004bm,Gregory:2006tu}.
This and decades of previous biological research provided a very basic view
of connections within the cell, yet many of the studies delineating
functions were limited to small sets of genes and proteins.
To be able to understand how all the components affect each other,
there had to be a way to take snapshots of the levels of thousands more 
mRNAs or proteins.

In 1982, technology to simultaneously measure
genome-wide gene expression (i.e., the levels of mRNA in a cell)
from a collection of cells was already being developed \cite{Augenlicht:1982wo}. 
Current RNA sequencing technologies
can now count individual mRNA molecules (\textit{transcripts}) from a collection of cells
for \${}1000 or less.

\subsection{Measuring mRNA levels with microarrays}

DNA molecules consist of two, connected, parallel strands, each strand 
containing a sequence of nucleobases along a sugar backbone.
The four bases (adenosine (A), thymine (T), cytosine (C), guanine (G))
join to each other by hydrogen bonds. A only pairs with T; C only pairs with G.
When two strands are aligned so that the base pairs match, the two strands
\textit{hybridize}.

Strand-specific hybridization can be used to identify
DNA sequences from uncharacterized samples. For example, single-stranded DNA
with a known sequence
can be fixed to a substrate or surface, and DNA from an unknown
source can then be labeled and washed over that surface.
If DNA from the two sources have matching strands (i.e., they
\textit{complement} one another), the labeled
DNA will hybridize and be detected.
The first ``gene arrays'' that could detect multiple sequences in this way
were built by attaching 
DNA to hundreds of spots (\textit{probes}) on glass plates or 
slides \cite{Maskos:1992co, Augenlicht:1982wo}.
Each probe contained thousands or millions of DNA 
molecules with the same sequence so that one gene was detected per probe.

Since hybridization requires two DNA samples, mRNA must
be reverse transcribed back to DNA if it is to be measured on a gene array.
The resulting complementary DNA (cDNA)
can then be labeled and detected.
Signal intensities from the probes indicate the relative amounts of each
mRNA in a sample.


Microarrays, very small gene arrays, were introduced in 1995 \cite{Schena:1995fy}. 
Although microarrays require more sophisticated
manufacturing, they are based on the principles of older gene arrays.


\section[Microarray preprocessing]{Microarray preprocessing techniques}

The most commonly used microarrays over the past decade
have been made by Affymetrix. I described here many methods
designed for these arrays, yet the principles can be extended to
most other microarray technologies and even sequencing data.

\subsection{Steps of data preprocessing}\label{introjtm:steps}

\subsubsection{Affymetrix microarrays}

In less than one square inch, Affymetrix arrays fit over one million probes, enough
to measure genome-wide expression (the \textit{transcriptome}).
Since exons are longer than the 25-nucleotide probes,
\textit{probe sets} of ten to fifteen probes are designed to hybridize
one gene or exon.

\subsubsection{Nonspecific hybridization}

For each probe sequence, Affymetrix made a
\textit{mismatch} probe with the 13\textsuperscript{th} 
base changed. 
By subtracting the mismatch signal from the \textit{perfect-match} signal,
\textit{nonspecific hybridization} (\textit{cross-hybrdization} 
from other transcripts) may be 
estimated (\textit{perfect-match correction}).
However, mismatch hybridization is complex.
Usually, more than one third of mismatch probes have a higher
signal than their perfect match probes \cite{Irizarry:2003ge}. 
This should theorectically never happen. Several have proposed
hierarchical models with nonlinear terms that
better account for mismatch probes 
\cite{Li:2001jv,Milo:2003tt,Liu:2005ey,Hein:2005ip}, yet algorithms
that ignore mismatch probes perform equally
well or better \cite{Chen:2007cr,Irizarry:2003ge,Hochreiter:2006ja}.

\subsubsection{Background correction}

Since mismatch probes cannot be used and there is no 
empty space on high-density arrays, 
background signals must be estimated from many
probes. Affymetrix first proposed splitting an array
into zones and calculating background signals from low
intensity probes. This approach systematically corrects large
sections of the array,
yet it does not address probe-specific background signals.

Taking a statistical approach, 
Irizarry et al. observed the distribution
of all observed probe signals ($O$) could be approximated by
a mixture of an exponential distribution ($S$) and a
normal distribution ($B$) \cite{Irizarry:2003ge}.
$S$ and $B$ were considered the true signal and background signal, respectively ($O=S+B$).
After the mean and variance of $S$ and $B$ are estimated from the data, 
the background-corrected signal can be calculated 
as $\text{E}[S|O=o]$ by the robust multi-array average (RMA) procedure in
\cite{Irizarry:2003ge}. 
Wu et al. introcuded gcRMA, which improved upon RMA by 
accounting for sequence-specific probe
affinities that were determined from previous experiments
($O=S+B+N$ where $N$ is non-specific hybridization due to
differences in probe sequences) \cite{Wu:2004wh}. gcRMA
also modeled mismatch probes by making two equations, one for 
$O_{\text{mismatch}}$ and one for $O_{\text{perfect}}$, with one common 
term, the true signal $S$. 
Similarly, other model-based expression value calculations
have the option to include or ignore
mismatch probes (e.g.,\cite{Li:2001jv,Milo:2003tt}).

\subsubsection{Probe set summarization}

To estimate a gene's expression, the probes in a probe
set must be summarized. 
Since outliers are common, robust
statistics (e.g., median) are preferred.
Affymetrix first recommended the Tukey bi-weight
statistic, calculating probe set values one microarray at a time.
However, many probes have similar effects across all microarrays
(e.g., different affinities), and these \textit{probe effects} can 
be modeled and removed as was shown by Li and Wong 
(\cite{Li:2001jv,Li:2001wk} is often called the ``Li-Wong method'').
The most popular summarization, ``median polish'', places a probe set's expression
values $a_{ij}$ in a matrix ($i$ indicates the probe an $j$ 
indicates the array). The row and column medians are 
iteratively subtracted to estimate an error matrix. Probe effects and
expression values are calculated from the sum of the subtracted row 
and column medians, respectively, minus the 
medians of both vector sums \cite{Mosteller:1977vp,Irizarry:2003ge}.  
Chen et al.'s ``distribution-free, weighted'' summarization
accounts for probe effects primarily by assuming that low-variability
probes are high-quality and should be weighted more 
than other probes \cite{Chen:2007cr}.
Hochreiter et al. assume perfect match probes are normally distributed, enabling
them to perform `factor analysis' 
where the `factors'
are mRNA concentrations \cite{Hochreiter:2006ja}. 
Even more complex models may incorporate 
background correction , perfect-match correction, and probe set summarization
into one step (e.g. \cite{Zhang:2003to,Liu:2005ey}), 
yet each step is typically performed separately.

\subsubsection{Normalization}

Systematic differences between arrays must be normalized if they
are to be compared. The simplest normalization procedures center
all array values by mean, median, or some other measure, yet centering
doesn't account for different ranges of values.
Quantile normalization forces two arrays to have the exact same
statistical distribution \cite{Bolstad:2003ia}. Li and Wong's normalization
iteratively searches for a group of ``householding genes'' (the invariant 
set of probes) that will be forced to the same expression values in all arrays,
and all other probes are adjusted accordingly \cite{Li:2001wk}.
Huber et al. found that the inverse hypberbolic sine transformation
of array data made a probe's variance across arrays 
less dependent on its mean \cite{Huber:2003bw}.
This variance stabilization---in combination with a nonlinear model that is fit to find
scaling factors and offsets for each microarray---is used for normalization.
Loess normalization applies a smoother to an `MA plot' which plots
the differences between two arrays ($M=log_2(x_1/x_2)=log_2(x_1)-log_2(x_2)$)
versus the average signal of two arrays ($A=\frac{1}{2}log_2(x_1x_2)=\frac{1}{2}(log_2(x_1)+log_2(x_2))$).
The smoother is subtracted from each point so that the plot
is centered around the $A$-axis. Loess normalization thus makes
offset adjustments that are dependent on the intensity of the signal.

\subsection{Order and necessity of analysis steps}\label{introjtm:steporder}

All microarray preprocessing steps are optional.
 
If one is sure of impeccable sample isolation and
reproducibility, they might decide against normalization. 
This rarely happens in practice. Only in some
\textit{in vitro} experiments performed at the same time by the same person
are samples so similar that skipping normalization might be unnecessary.
However, even the slightest differences in one of many experimental steps 
(e.g., scanner reproducibility, mRNA concentratioin calculations, hybridization
temperatures) usually cause systematic errors that require normalization.
There should be strong justification for skipping normalization.

Background correction algorithms essentially deconvolute the observed
signal into two signals: the noise and true signal.
They are most helpful then for low-signal probes where
the signal to noise ratio is lowest. If one is only interested in
probes with the highest signal, they might consider skipping
background correction. After all, background correction is a model, and models
are always wrong.

One may trust that mismatch probes entirely account for
background signals and nonspecific hybridization. 
However, mismatch and perfect match
probes are usually correlated, indicating that mismatch probes still
weakly detect the target transcript. Therefore mismatch probes should
be ignored or included as a factor in statistical models that perform
background correction.

Probe set summarization may even be skipped, and expression values
for 500,000+ probes reported by aligning each probe to a gene. Summarization
reduces 10+ probes to one value, so 90\% of the information content
is lost. With so much focus on data reduction, this
dramatic loss in information is overlooked. Before summarization, one might use
the distribution of probe values to perform more powerful statistical tests or to propogate
error through subsequent analyses \cite{Milo:2003tt,Liu:2005ey}.
Nevertheless, probe set summarization must eventually be performed
at some level if one wants to study genes, not 25-nucleotide stretches of DNA.

There is no strict requirement for the order in which the steps are performed,
yet there are limitiations. For instance, if probe set summarization were done first,
then background summarization could not use the probe values to estimate
the distribution of noise in the array. Perfect-match correction also wouldn't
be possible. Probe set summarization is usually the last step, and background
correction is the first.

Normalization can be applied before and/or after summarization.
There is no strong evidence supporting one choice over the other.
It is also unclear if normalization should be done for all arrays at once
or separately for subsets (e.g., control and treatment groups).

\section{Choosing preprocessing techniques}

\subsection{What are the best preprocessing steps?}

Usually, the answer is ``we're not sure'' or ``it depends''. 
Hundreds of methods have been published.
Which ones are chosen depends on if the method's assumptions (many
discussed in \ref{introjtm:steps}) match
the experimental design.

Selecting the full sequence of steps (the \textit{workflow}) is daunting.
The techniques in \ref{introjtm:steps} are a subset of the many choices.
For each step (background correction, normalization,
perfect-match correction, and probe set summarization), there are
ten or more possible algorithms making at least $10^4=10,000$ possible
workflows. However, each algorithm has at least one, sometimes
three or four arbitrarily or heuristically chosen parameters, making
for over ten million possible workflows (actually real number parameters make 
for infinite workflows).
Some recommendations are given in \ref{introjtm:steporder}.
However, since there are no easy general best methods, I present an example. 
Consider an experiment with a cell line that has
treatment (e.g., a Rho inhibitor) and control microarrays. 

\subsubsection{Background correction and perfect-match correction}
Background correction is the least understood step, largely because
``there is currently no way to design 
an oligonucleotide microarray such that the probes have 
fully predictable hybridization'' \cite{Pozhitkov:2007go}.
gcRMA and PDNN use sequence data to try and infer complex
probe affinities from sequence data yet are not much better than
some that do not \cite{Irizarry:2003ge,Zhang:2003to,Wu:2004wh}.
Most methods make a model of the form $\text{observed}=f(\text{signal})
+ g(\text{noise}) + \text{constant}$. $f$ and $g$ are either statistical
distributions or nonlinear terms whose parameters are fit
by estimates such as maximum likelihood. 
Affymetrix's MAS5.0 background correction and perfect-match corrections
are not model based, making simple assumptions on how low-intensity probes
or mismatch probes should adjust surrounding perfect-match probes.
There is no clear, universal support
of one method over another. However, it is commonly accepted that 
mismatch signals should be ignored or modeled as partially 
contributing to the observed signal.

\subsubsection{Normalization}
If the researchers believe that only a few dozen
of thousands of transcripts will be affected, they might assume that the
probe value disributions are similar among all microarrays. Quantile
normalization might then be appropriate. If treatment
systematically increases the total mRNA per cell, quantile
normalization would incorrectly mean-center all arrays 
(though uncommon, cells may need to be counted before RNA isolation \cite{Loven:2012km}).
Loess normalization might be better since it 
modifies each array's values according to similarly expressed 
probes on other arrays, allowing for slightly different distributions.
If the primary problem is high variance of low-signal probes, variance
stabilization may be best. If $~$10 ``householding'' genes are known
or can be trusted to be found algorithmically, invariant set normalization
would work. However, since invariant set normalization assumes probe affinities
are similar, probe values must be corrected in background correction.
Treatment groups may be so different that
no normalization cannot be justified. The treatment groups might then
be separately normalized, although subsequent comparisons may be difficult to interpret.

\subsubsection{Probe set summarization}


\subsubsection{Different choices for different goals}

For instance, Zhang et al claimed their PDNN model 
was superior to dChip ("Li-Wong" method)
and MAS5.0 (Affymetrix default) \cite{Zhang:2003to}.
However, Wu and Irizzary quickly published a comment on 
showing that their
RMA and gcRMA methods performed as well or better for predicting
mRNA concentrations in spike-in experiments \cite{Wu:2004ul}.
Zhang et al. responded that their concentration predictions
are off only in a scale factor that can easily be estimated, and that
sensitivity and specificity to detect differentially expressed genes
were slightly better than RMA and gcRMA \cite{Zhang:2004tl}. They were also unable to
reproduce results supporting Wu and Irizarry's claims. This
debate exemplified the misunderstandings in microarray analysis.
It was unclear what the goal should be: accurate predictions of mRNA levels or
differentially expressed genes?

If the goal is correlation, than gcRMA should not be done.


\subsubsection{The gold standard}

In an effort to once and for all determine the best
preprocessing methods, Zhu et al. designed a ``gold standard''
experiment



Golden Spike
Platinum Spike



\subsection{Which preprocessing steps should I choose?}

More than one. The one that matches with your goals.

Over ten years of 

\subsection{Interpreting processed microarray data}

some work for better output. correlation with RMA versus gcRMA
gcRMA sacrifices precision for better accuracy

\subsubsection{How data processing affects results}

The truth is that some work better for detecting levels, some
better account for low intensity variation, some find a way use the mismatch
information to gain a slight boost in accuracy. There is no best way.



\section[Differential expression]{Detecting differentially expressed genes}


\section{Caveats of microarrays and alternatives}

Most of the problems with microarrays are low-intensity probes or
low-abundance transcripts. Discussion.

The second problem is that we don't understand how transcripts
interact with surfaces. These problems can hopefully be avoided
with RNA sequencing.

\section{Future improvements in transcriptomics analyses}

There are even more caveats to microarrays than the ones presented
above. Pozhitkov et al. review many of these limitations and go
so far as stating that ``there is currently no way to design 
an oligonucleotide microarray such that the probes have 
fully predictable hybridization'' \cite{Pozhitkov:2007go}.


The greatest need is visualization, Culhane et al provide a visualization
for data sets from different studies and platforms \cite{Culhane:2003it}.
Le Cao improved upon Culhane et al \cite{Cao:2009dd}.
Other methods have been made to correct for cross-platform differences \cite{Fan:2011iy}.

Ensemble methods
It is possible that data from different platforms could be combined
to provide additional information \cite{Hockley:2009tf}.
Models may also be combined as they were in predictive models
of cancer diagnostics \cite{Chen:2011ib}.

very important to have the correct probe annotations \cite{Carter:2005bq}.

\section{Visualizing results}

\section{Reproducibile analyses}

\subsection{Why bother?}

Irreproducible analyses are dangerous, perhaps bordering on unethical negligence. 
Dave et al. in the \textit{New England
Journal of Medicine} reported a marker for follicular lymphoma \cite{Dave:2004vl}. 
In letters to the editor, Tibshirani and Hong et al. stated they 
could not reproduce the analyses \cite{Tibshirani:2005el}. 
Dave et al. rectified the discrepancy as a misunderstanding
in how their data was interpreted. This is just one of a handful of
public disputes in the literature about gene expression analyses (two mentioned
in previous paragraphs).
Authors of disputed studies are most always well-intentioned, yet 
irreproducible analyses or poorly presented data raise suspicions.

In my opinion, analysts shouldn't fear being wrong; most researchers
are wrong most of the time. They must fear being overconfident or misleading. 
By making data and code open to criticism,
scientists protect their integrity and intellectual property in the same
way that lab notebooks do. They protect their colleagues
whose careers are dependent on their analyses as well as the patients
whose health decisions are affected by their research.
Finally, they increase there chance for mutually beneficial collaborations.

\subsection{Are microarrays reproducible?}

Thre has been great concern about inter-lab repeatability of 
gene expression experiments
(see example of poor reproducibility in 
\cite{Evsikov:2003ic,Fortunel:2003eu,Ivanova:2003bh,RamalhoSantos:2002dy}). 
Rather than discrediting microarray analysis,
one should also consider that micorarrays
may reveal differences in \textit{seemingly} same experimental protocols.
For example, with the data presented in this dissertation, arrays from
replicate experiments on different days were clearly different when
visualized with principal components analysis. However, the differences
were systematic and could be corrected.

Problems may also arise from overexpectations or and misunderstanding 
of what expression values represent. 
For example, comparisons of microarray classification
studies (e.g., \cite{Veer:2002vv,Yeoh:2002wg,Alizadeh:2000tn,Golub:1999fn,
Perou:1999fr,Wang:2005uu,VanDeVijver:2002wj,Rosenwald:2002eg,Beer:2002uy,
Bhattacharjee:2001dt,Ramaswamy:2003to,Pomeroy:2002wx,Iizuka:2003wo})
with non-microarray studies have indicated predictive cancer markers or profiles
may not be as reliable as hoped \cite{Miklos:2004wn,Michiels:2007vs,Koscielny:2010gg,Eden:2004ua}.
Hundreds or even thousands of microarrays may be necessary to
accurately predict cancer 
outcomes \cite{Ioannidis:2005kd,Michiels:2005tg,EinDor:2006ga,Frantz:2005ur}.
However, a more rigorous re-evaluation, of a pessimistic study claiming
that microarray studies cannot predict cancer markers, found that
markers can indeed be found (see \cite{Michiels:2005tg,Fan:2010cv,Tong:2010ka,Koscielny:2010jd}
for another collegial debate).
Thus, one shouldn't jump to conclusions from any one or even a few
analytical workflows.
For instance, two studies may find very different
list of differentially expressed genes, yet the correlation
between the data sets may be strong. Alternatively, different studies may
predict different markers that are all correct \cite{Zhang:2008bk,Zhang:2009cy}.

Several studies 
raise concerns about experimental reproducibility between
experimental platforms \cite{Tan:2003be,Kuo:2002cl,
Mah:2004ia,Rogojina:2003te,Woo:2004wz,Li:2002cz,Kothapalli:2002gz}.
Many irreproducibility claims were incorrect, first
dismissed by scientsist at the FDA \cite{Shi:2005ik}.
The FDA and EPA coordinated a Microarray Quality Control (MAQC) project
to set standards and resolve outstanding questions 
\cite{Lesko:2004vi,Frueh:2006uy,Dix:2006ty,Shi:2004vh}.
With careful quality assurance and analyses,
microarray data were similar between 
laboratories \cite{Dobbin:2005wj,Irizarry:2005kb,Larkin:2005hb,
Ulrich:2004vw,Waring:2004vh,Weis:2005ut} and
microarray platforms \cite{Petersen:2005by,Yauk:2004fq,
Park:2004ux,Yuen:2002ha,Canales:2006ts,Shippy:2006uf,
Patterson:2006hi,Tong:2006cg,Guo:2006wi}.

\subsection{How to make reproducible analyses?}

\subsection{Reproducibility of this dissertation}

Being wrong as a scientist is expected, but being wrong in a way
that other


disput




