
\chapter[Choosing transcriptomics analyses]{Reliable, reproducible 
           transcriptomics analyses}\label{chapter:introjtm}

Throughout this dissertation, I present analyses of the expression of the genomes
of human cells or tissues of mice that have been treated 
with \textit{C. difficile} toxins A and B
(TcdA and TcdB) in order to determine the pathogenic responses
of the host at the cellular level. 
In learning how to process this type of data,
I encountered many limitations and misunderstandings in 
common gene expression analyses. In this chapter, I describe
the background of this type of data and data analyses that form 
the base of this dissertation, and explain some important considerations
for interpreting sometimes very different results 
produced by different data analyses, an overlooked problem in the
majority of gene expression studies.

\section{Background}

\subsection{mRNA as a measure of cell state}

Our genomes are a sequence of $\sim$3-billion ``letters'' from a four-letter
alphabet of nucleobase molecules (\textit{bases}) \cite{Lander:2001wi}. 
Each of our $\sim$20,000 genes is pieced together from, on average,
5 physically separate sequences called \textit{exons}. Exons, which
range from $\sim$20 to 1,600 bases, are contained
within one of the 46 strands of DNA in our cells \cite{Michael:1999dm}.

The central dogma of molecular biology is 
DNA $\rightarrow$ RNA $\rightarrow$ protein \cite{Crick:1970wb}.
In each cell, exons are
copied (\textit{transcribed}) and spliced into portable
messenger RNA (mRNA). 
mRNA is then \textit{translated} to strings
of amino acids that arrange themselves into shapes with 
chemical properties that perform specific tasks.
These amino acid strings, or proteins, are the primary functional 
units that execute the instructions in our DNA.

When a protein is needed, a cell's ``circuitry'' 
triggers a gene (the DNA encoding that protein)
to be transcribed to mRNA that is then translated to the protein.
Specific amounts of proteins are produced
in response to different stimuli. Since the proteins will alter
the physical state of cells and consequently the body's overall physiological responses, 
many scientists have striven to understand the arrangement and logic
of this regulatory circuitry.

Most of this circuit's components were identified soon after the
sequencing of the human genome in 2003 \cite{Consortium:2004bm,Gregory:2006tu}.
This and decades of previous biological research provided a very basic view
of connections within the cell, yet many of the studies delineating
functions were limited to small sets of genes and proteins.
To be able to understand how all the components affect each other,
there had to be a way to take snapshots of the levels of thousands more 
mRNAs or proteins.

In 1982, technology to simultaneously measure
genome-wide gene expression (i.e., the levels of mRNA in a cell)
from a collection of cells was already being developed \cite{Augenlicht:1982wo}. 
Current RNA sequencing technologies
can now count individual mRNA molecules (\textit{transcripts}) from a collection of cells
for \${}1000 or less.

\subsection{Measuring mRNA levels with microarrays}

DNA molecules consist of two, connected, parallel strands, each strand 
containing a sequence of nucleobases along a sugar backbone.
The four bases (adenosine (A), thymine (T), cytosine (C), guanine (G))
join to each other by hydrogen bonds. A only pairs with T; C only pairs with G.
Two strands \textit{hybridize} when their base pairs are aligned.

Strand-specific hybridization can be used to identify
DNA sequences from uncharacterized samples. For example, single-stranded DNA
with a known sequence
can be fixed to a substrate or surface, and DNA from an unknown
source can then be labeled and washed over that surface.
If DNA from the two sources have matching strands (i.e., they
\textit{complement} one another), the labeled
DNA will hybridize and be detected.
The first ``gene arrays'' that could detect multiple sequences in this way
were built by attaching 
DNA to hundreds of spots (\textit{probes}) on glass plates or 
slides \cite{Maskos:1992co, Augenlicht:1982wo}.
Each probe contained thousands or millions of DNA 
molecules with the same sequence so that one gene was detected per probe.

Since hybridization requires two DNA samples, mRNA must
be reverse transcribed back to DNA if it is to be measured on a gene array.
The resulting complementary DNA (cDNA)
can then be labeled and detected.
Signal intensities from the probes indicate the relative amounts of each
mRNA in a sample.


Microarrays, very small gene arrays, were introduced in 1995 \cite{Schena:1995fy}. 
Although microarrays require more sophisticated
manufacturing, they are based on the principles of older gene arrays.


\section[Microarray preprocessing]{Microarray preprocessing techniques}

The most commonly used microarrays over the past decade
have been made by Affymetrix. I described here many methods
designed for these arrays, yet the principles can be extended to
most other microarray technologies and even sequencing data.

\subsection{Steps of data preprocessing}\label{introjtm:steps}

\subsubsection{Affymetrix microarrays}

In less than one square inch, Affymetrix arrays fit over one million probes, enough
to measure genome-wide expression (the \textit{transcriptome}).
Since exons are longer than the 25-nucleotide probes,
\textit{probe sets} of ten to fifteen probes are designed to hybridize
one gene or exon.

\subsubsection{Nonspecific hybridization}

For each probe sequence, Affymetrix made a
\textit{mismatch} probe with the 13\textsuperscript{th} 
base changed. 
By subtracting the mismatch signal from the \textit{perfect-match} signal,
\textit{nonspecific hybridization} (\textit{cross-hybrdization} 
from other transcripts) may be 
estimated (\textit{perfect-match correction}).
However, mismatch hybridization is complex.
Usually, more than one third of mismatch probes have a higher
signal than their perfect match probes \cite{Irizarry:2003ge}. 
This should theoretically never happen. Several have proposed
hierarchical models with nonlinear terms that
better account for mismatch probes 
\cite{Li:2001jv,Milo:2003tt,Liu:2005ey,Hein:2005ip}, yet algorithms
that ignore mismatch probes perform equally
well or better \cite{Chen:2007cr,Irizarry:2003ge,Hochreiter:2006ja}.

\subsubsection{Background correction}

Since mismatch probes cannot be used and there is no 
empty space on high-density arrays, 
background signals must be estimated from many
probes. Affymetrix first proposed splitting an array
into zones and calculating background signals from low
intensity probes. This approach systematically corrects large
sections of the array,
yet it does not address probe-specific background signals.

Irizarry et al. observed the distribution
of all observed probe signals ($O$) could be approximated by
a mixture of an exponential distribution ($S$) and a
normal distribution ($B$) \cite{Irizarry:2003ge}.
$S$ and $B$ were considered the true signal and background signal, respectively ($O=S+B$).
After the mean and variance of $S$ and $B$ are estimated from the data, 
the background-corrected signal can be calculated 
as $\text{E}[S|O=o]$ by the robust multi-array average (RMA) procedure in
\cite{Irizarry:2003ge}. 
Wu et al. introduced gcRMA, which improved upon RMA by 
accounting for sequence-specific probe
affinities that were determined from previous experiments
($O=S+B+N$ where $N$ is non-specific hybridization due to
differences in probe sequences) \cite{Wu:2004wh}. gcRMA
also modeled mismatch probes by making two equations, one for 
$O_{\text{mismatch}}$ and one for $O_{\text{perfect}}$, with one common 
term, the true signal $S$. 
Similarly, other model-based expression value calculations
have the option to include or ignore
mismatch probes (e.g.,\cite{Li:2001jv,Milo:2003tt}).

\subsubsection{Probe set summarization}

To estimate a gene's expression, the probes in a probe
set must be summarized. 
Since outliers are common, robust
statistics (e.g., median) are preferred.
Affymetrix first recommended the Tukey bi-weight
statistic, calculating probe set values one microarray at a time.
However, many probes have similar effects across all microarrays
(e.g., different affinities), and these \textit{probe effects} can 
be modeled and removed as was shown by Li and Wong 
(\cite{Li:2001jv,Li:2001wk} is often called the ``Li-Wong method'').
The most popular summarization, ``median polish'', places a probe set's expression
values $a_{ij}$ in a matrix ($i$ indicates the probe an $j$ 
indicates the array). The row and column medians are 
iteratively subtracted to estimate an error matrix. Probe effects and
expression values are calculated from the sum of the subtracted row 
and column medians, respectively, minus the 
medians of both vector sums \cite{Mosteller:1977vp,Irizarry:2003ge}.  
Chen et al.'s ``distribution-free, weighted'' summarization
accounts for probe effects primarily by assuming that low-variability
probes are high-quality and should be weighted more 
than other probes \cite{Chen:2007cr}.
Hochreiter et al. assume perfect match probes are normally distributed, enabling
them to perform `factor analysis' 
where the `factors'
are mRNA concentrations \cite{Hochreiter:2006ja}. 
Even more complex models may incorporate 
background correction , perfect-match correction, and probe set summarization
into one step (e.g. \cite{Zhang:2003to,Liu:2005ey}), 
yet each step is typically performed separately.

\subsubsection{Normalization}

Systematic differences between arrays must be normalized if they
are to be compared. The simplest normalization procedures center
all array values by mean, median, or some other measure, yet centering
doesn't account for different ranges of values.
Quantile normalization forces two arrays to have the exact same
statistical distribution \cite{Bolstad:2003ia}. Li and Wong's normalization
iteratively searches for a group of ``householding genes'' (the invariant 
set of probes) that will be forced to the same expression values in all arrays,
and all other probes are adjusted accordingly \cite{Li:2001wk}.
Huber et al. found that the inverse hypberbolic sine transformation
made probe variance less dependent on probe mean \cite{Huber:2003bw}.
This variance stabilization---in combination with a nonlinear model fit to find
scaling factors and offsets for each microarray---is used for normalization.
Loess normalization applies a smoother to an `MA plot' which plots
the differences between two arrays ($M=log_2(x_1/x_2)=log_2(x_1)-log_2(x_2)$)
versus the average signal of two arrays ($A=\frac{1}{2}log_2(x_1x_2)=\frac{1}{2}(log_2(x_1)+log_2(x_2))$).
The smoother is subtracted from each point so that the plot
is centered around the $A$-axis. Loess normalization thus makes
offset adjustments that are dependent on the intensity of the signal.


\section{Choosing preprocessing techniques}

\subsection{What are the best preprocessing steps?}\label{introjtm:whatsbest}

Usually, the answer is ``we're not sure'' or ``it depends''. 
Hundreds of methods have been published.
Which ones are chosen depends on if the method's assumptions match
the experimental design.

Selecting the full sequence of steps (the \textit{workflow}) is daunting.
The techniques in \ref{introjtm:steps} are a subset of the many choices.
For each step (background correction, normalization,
perfect-match correction, and probe set summarization), there are
ten or more possible algorithms making at least $10^4=10,000$ possible
workflows. However, each algorithm has at least one, sometimes
three or four arbitrarily or heuristically chosen parameters, making
for over ten million possible workflows (actually real number parameters make 
for infinite workflows).
Since there are no easy general guidelines, I present some
illustrative examples below. 

\subsubsection{Background correction and perfect-match correction}

Background correction algorithms decompose the observed
signal into two signals: the noise and true signal.
They are most helpful then for low-signal probes where
the signal to noise ratio is lowest. If researchers are uninterested
in low-abundance transcripts, they might consider skipping
background correction.

Background correction is the least understood step because
``there is currently no way to design 
an oligonucleotide microarray such that the probes have 
fully predictable hybridization'' \cite{Pozhitkov:2007go}.
gcRMA and PDNN use sequence data to try and infer complex
probe affinities from sequence data yet are not much better than
some that do not \cite{Irizarry:2003ge,Zhang:2003to,Wu:2004wh}.
Affymetrix's MAS5.0 background correction and perfect-match corrections
are not model based, making simple assumptions on how low-intensity probes
or mismatch probes should adjust surrounding perfect-match probes.
Although there is no clear, universal support of one method over another,  
it is commonly accepted that 
mismatch signals should be ignored or modeled in some way as 
contributing to the observed signal.

\subsubsection{Normalization}

If one is sure of impeccable sample isolation and
reproducibility, they might decide against normalization. 
However, since even the slightest differences in one of many experimental factors 
(e.g., scanner reproducibility, mRNA concentration calculations, hybridization
temperatures) cause systematic errors, there should be strong 
justification for skipping normalization.

If researchers believe that only a few dozen
of thousands of transcripts vary between arrays, then the distribution
of expression values should be similar among all arrays. Quantile
normalization would then be appropriate. If treatment
systematically increases the total mRNA per cell, quantile
normalization would incorrectly mean-center all arrays 
(though uncommon, cells may need to be counted before RNA isolation \cite{Loven:2012km}).
Loess normalization might be better since it 
modifies each array's values according to similarly expressed 
probes on other arrays, allowing for slightly different distributions.
If the primary problem is high variance of low-signal probes, variance
stabilization may be best. If $~$10 ``householding'' genes are known
or can be trusted to be found algorithmically, invariant set normalization
would work. However, since invariant set normalization assumes probe affinities
are similar, probe values must be corrected in background correction.
Treatment groups may be so different that
no normalization cannot be justified. The treatment groups might then
be separately normalized, although subsequent comparisons may be difficult to interpret.

\subsubsection{Probe set summarization}

There are no general rules for probe set summarization, yet
there are mistakes to be avoided. Outliers are common in microarrays
so robust measures of center are used.
It is recommended to choose summaries
that ``borrow'' information from all arrays to identify probe effects.
Though different summarization techniques may produce significantly
different expression values, no technique is necessarily incorrect.
However, the artifacts introduced by some techniques may
cause incorrect interpretations in downstream analyses (see \ref{introjtm:difgoals}).

Summarization reduces 10+ probes to one value, so 90\% of the data
is lost. Therefore, before summarization, one might use
the distribution of probe values to perform more powerful statistical tests or to propogate
error through subsequent steps \cite{Milo:2003tt,Liu:2005ey}.
Nevertheless, probe set summarization must eventually be performed
at some level if one wants to study genes, not 25-nucleotide stretches of DNA.

\subsubsection{The order of steps}
There is no required order for each step of the workflow,
yet there are limitations. For instance, if probe set summarization were done first,
probe values would not be available to estimate the background signal. 
Perfect-match correction also wouldn't
be possible. Normalization can be applied before and/or after summarization.
There is no strong evidence supporting one choice over the other.
It is also unclear if normalization should be done for all arrays at once
or separately for subsets (e.g., control and treatment groups).

With few exceptions, each preprocessing method is modular, compatible
with any other method. However, very few have explored
the effects effects of combining algorithms.
For example, it may be the case that a background correction
invalidates assumptions for some normalization procedures.

\subsubsection{Different choices for different goals}\label{introjtm:difgoals}

Different analyses work better for different problems.
For example, Zhang et al claimed their PDNN model 
was superior to dChip ("Li-Wong" method)
and MAS5.0 (Affymetrix default) \cite{Zhang:2003to}.
However, Wu and Irizzary commented that their
RMA and gcRMA methods performed as well or better for predicting
mRNA concentrations \cite{Wu:2004ul}.
Zhang et al. responded that their concentration predictions
are off by a predictable scale factor, and that
the ability to detect differentially expressed genes
was better than RMA and gcRMA \cite{Zhang:2004tl}. They were also unable to
reproduce results supporting Wu and Irizarry's claims.
It was unclear what the goal should be: accurate predictions of mRNA levels or
differentially expressed genes?

If the goal of a study is to compare the profiles of many genes
or samples, one must be aware of artifacts introduced by
probe set summarization algorithms that severely
overestimate correlations.
Lim et al. observed, with gcRMA, an average correlation coefficient of 0.4
among randomly generated, uncorrelated arrays \cite{Lim:2007gc}. 
Since correlation measures are essential
for reverse engineering regulatory networks, previous network
studies that used gcRMA were flawed. 
Giorgi et al. identified that the median polish algorithm introduces 
high inter-sample correlations among randomly generated arrays \cite{Giorgi:2010jw}.
Their solution was to transpose the matrix of probe intensities
for each probe set, thereby transferring the error so that probes
would be overly correlated, not samples.
Therefore, if a study's goal is to classify patients by a clustering
algorithm, one should use the median polish algorithm very carefully.
If the goal is only to detect differentially expressed genes, then
the algorithm will not cause critical errors.


\subsubsection{Gold standards?}

To once and for all determine the best
preprocessing methods, Choe et al. spiked in
5,700 transcripts at various concentrations 
on 18 arrays (called ``Golden Spike'' \cite{Choe:2005dd})
By quantifying the accuracy of predicted differentially expressed genes, they defined
one best performing workflow, though several others 
performed similarly well. Several authors
noted flaws in the experimental design and analysis
causing unrealistic conclusions \cite{Pearson:2008jr,Schuster:2007wr,
Dabney:2006ud,Irizarry:2006bq,Gaile:2007ei,Fodor:2007wb}, and
a ``Platinum Spike'' data set was generated to address the flaws \cite{Zhu:2010jxa}.
Conflicting recommendations from various authors that stemmed from
these data suggests 
that there will likely never be one ``best'' workflow, yet the analyses
made possible by these data sets
highlighted advantages and pitfalls each method (many of which
are discussed in \ref{introjtm:whatsbest}) and led to the invention of new techniques.

\subsection{Which workflow do I use?}

The previous sections have shown that I or anyone
else cannot definitively choose the right workflow. 
Instead, I would recommend an exploratory approach 
tailored for each data set.
For example, one could analyze there data set with ten 
or more different workflows and compare the results
using diagnostic tools (e.g., correlation matrices, clustering,
principal components analysis (PCA), and MA plots).
For example, one may find, as I did in one case, that invariant set
normalization causes outlier arrays (identified by PCA) because
of the automatically selected ``householding genes''.
Using an MA plot, they may then notice extraordinarily
high variance in the fold changes of low-signal probes, and then decide
on a workflow (e.g., mmGmos) that propagates this error to
statistical tests for differential expression. If expression levels
from arrays will be used as parameters in another model (e.g., a model
of metabolic flux where expression levels indicate the presence of 
different enzymes), then the high-variance, low-signal expression values
might all be set to a common threshold.
Interactive, easy-to-use diagnostic visualizations that allow for these
decisions are desperately lacking.
Although developing visualizations is a tremendous technical challenge,
there is a great opportunity for improvement in this area.

\section[Differential expression]{Detecting differentially expressed genes}

After preprocessing, it is common to identify genes that are differentially
expressed (DEGs) between two treatment groups. Like preprocessing,
there are many methods with different assumptions (reviewed in
\cite{Cui:2003vl,Murie:2009dk,Storey:2003kd,Sreekumar:2008wj,
Pan:2002hy,Hong:2008hy,Yanofsky:2010by,Kadota:2011dz,Jeffery:2006bm}). This further expands
the number of possible workflows to well over 100 million.

Conceptually, significance tests for DEGs are simple.
For each gene, two groups can be compared with a t-test.
However, significant p-values are usually found for too many 
lowly expressed transcripts with small effect sizes because of very small
variances near the microarray detection limit.
Therefore, filtering of low-abundance transcripts is common, 
yet newer statistical tests can adjust for errors in low-signal probes
without excluding the probes from subsequent analyses.

\subsection{Types of statistical tests}

\subsubsection{Modified t-tests}\label{introjtm:modttests}

In Student's t-test, a t statistic is
the difference in expression between conditions (effect size) divided by the amount 
of variability in the data (standard error). The distribution of t-statistics
with different sample sizes is known, so how unusual (or how significant) a t-statistic is
can be calculated as a p-value.
Since the standard error of probes
is what causes too many significant transcripts, a simple solution
is to add a ``fudge factor'' to the standard error, thus making a
modified t-statistic.

\begin{equation}
t_{\text{modified}} = \frac{\text{Effect size}}{\text{fudge factor} + \text{standard error}}
\end{equation}

The goal of several bioinformatics studies has been to how to
best estimate the fudge factor. Tusher et al. heuristically 
chose a constant that minimized
the variation of the standard error across all
expression values \cite{Tusher:2001kk}.
Efron et al. chose the constant to be the 
90\textsuperscript{th} percentile of the standard
error for all transcripts \cite{Efron:2000wq,Efron:2001uu}.
Baldi and Speed's cyberT method
adds ``pseudo-replicate'' arrays for which the standard deviation
is estimated as the average standard deviation of similarly 
expressed genes on the real arrays \cite{Baldi:2001ul}.
The variance of expression values is therefore ``shrunk'' towards
the variance of similarly expressed genes.
Fox et al. instead calculate
the variance of pseudoreplicates using the sum-squared differences of similarly
expressed genes \cite{Fox:2006hq}.
Demissie et al. show how to use a similar fudge factor but for a 
Welch test (a t-test where groups have unequal variance) \cite{Demissie:2008dv}.

\subsubsection{Bayesian statistics}
cyberT improves upon a regular t-test by incorporating
information we know (or guessed) to be true, namely that the sample variance of low-signal
probes is usually greater than observed. ``Bayesian statistics'' is the field
of statistics that allows one to make such prior assumptions (called \textit{priors})
in a mathematically rigorous way
to reduce the set of possible outcomes (the sample space). The reduced sample
space allows us to make new, \textit{posterior} 
probabilities \textit{given} the prior information. For example, my
guess of the average height of people in a room (perhaps 5'6'') would
change dramatically if I were told prior that everyone 
was 2 years old (a subset of the population--the reduced sample space). Bayesian statistics
are common in microarray analyses. Since formulas
for prior and posterior probabilities can be esoteric, I will only
mention the assumptions on which the priors are based.

Lonnstedt et al. derived a statistic equal to the log of the 
probability a gene is a DEG divided by the probability the gene is not a DEG \cite{Lonnstedt:2002wu}.
To do so, they made the prior assumptions that (1) only a small proportion, $p$, of 
genes are DEG, (2) all fold changes of transcripts are normally distributed,
and (3) the variances of expression values follow an inverse gamma distribution.
The parameters for prior
distributions (e.g. the mean and variance of 
the normal distribution) are 
called hyperparameters. Lonnstedt et al. guessed $p$ and estimated
the other hyperparameters using the data. Efron et al.
assumed a prior distribution of a modified t-statistic
based on random permutations of microarrays. This ``empirical
Bayes'' procedure has been used in several other DEG tests \cite{Efron:2001uu}.
As researches have continued to learn about microarray chemistry so that
we may make better prior assumptions, Bayes statistics for DEG detection
have continued to be published. See
the aforementioned reviews and citations for more examples 
\cite{Newton:2001te,Murie:2008iz,Jain:2003kh,Yu:2011vo,
Astrand:2008jk,Newton:2004cp,Gottardo:2003hb,Efron:2002bz,Townsend:2002tf}.


\subsubsection{Linear models}
Linear models are a natural extension to t-tests when an experiment
has multiple factors that describe the samples (e.g., treatment group,
gender, RNA isolation protocol). Analysis of variance (ANOVA) is
used to estimate how much of the experiment-wide variance is 
due to each factor. Instead of a t-statistic, ANOVA finds an F-statistic
which compares these variances. Like the modified t-tests 
in \ref{introjtm:modttests} , there are
several modified F-tests, some of which use Bayesian statistics to
estimate fudge factors (reviewed in \cite{Cui:2003vl}).
For example, the IBMT method extends cyberT's assumptions to
linear models \cite{Sartor:2006hw}.
Perhaps the most common DEG test, ``linear models for
microarray data'' (LIMMA), builds a linear model based on multiple
factors that are then reduced to another linear model calculating
specific contrasts (effect sizes) \cite{Smyth:2005ht,Smyth:2004gh}. Limma extends the bayesian moderated
t-statistic from Lonnstedt et al. to calculate the signifcance 
of the contrasts for each gene \cite{Lonnstedt:2002wu}.


\subsubsection{Rank-based metrics}
t-tests and linear models (types of parametric tests) assume
the expression values for each gene are normally distributed. However, the
assumption may not be justified; there usually aren't enough samples to know.
Additionally, the assumption required by the t-test
that the mean and variance be independent is usually not true in microarray data.
Nevertheless, parametric tests are often chosen
for practical reasons. Expensive microarray experiments have small sample sizes, and
parametric tests are often needed to gain enough statistical power
to find DEGs.

However, several non-parametric tests have been developed which
make fewer assumptions \cite{Qi:2011tc}.
The Wilcoxan rank sums test uses combinatorics
to calculate how unusual the locations of treatment groups are when
ranking microarrays by expression values for each gene.
In another approach called RankProd, genes are ranked by fold change for all
two-array treatment group comparisons (e.g., $3\times3=9$ comparisons for triplicate 
samples in two treatment groups) \cite{Breitling:2004te,Breitling:2005ue,Hong:2006gf}.
For each gene, the product of its ranking in all comparisons is calculated.
To determine if
a rank product is unusual (significant), samples are permuted
between treatment groups many times to estimate the
usual distribution of rank products (the null distribution).


\subsubsection{Permutation tests}
Many other statistical tests use permutations to avoid
making inappropriate assumptions about the data (e.g., \cite{Yan:2005tc, Efron:2001uu}.
The most popular DEG test (by citation count) is Significant
Analysis of Microarrays (SAM) \cite{Tusher:2001kk}. SAM calculates moderated t-statistics
for each permutation and compares this to the actual moderated t-statistic
to estimate which genes fall below a specified 
false discovery rate (FDR).
The greatest limitation of permutation tests is that they
require much larger sample sizes than is typical in
costly microarray experiments. For example, the minimum
two-sided p-value for an 8-sample permutation test with quadruplicates
is only $2/{8 \choose 4}=0.03$. Hence although permutation tests
are possible with moderate sample sizes, it is better to have at 
least ten arrays where the minimum p-value is 0.008.


\subsubsection{Machine learning-based tests}
Many of the previously introduced statistics such as the 
modified t-statistic violate the assumptions on the significance tests
are based, or the test statistics are no longer in forms
that can be used in statistical tests.
Although these methods depart from the statistical
theory, they are still useful
for ranking and prioritizing genes. In this spirit,
some statisticians have used machine learning algorithms
to prioritize genes even though the numbers from the
algorithms are difficult to interpret. For example, Lu et al
performed PCA on the probes in a probe set to determine
which probe sets to filter as not DEGs \cite{Lu:2011ba}. They used the
loading on the first principal component relative to
all other loadings to set a filtering cutoff.
Clark et al. use linear discriminant analysis to find
a hyperplane in n-dimensional space (where n is the number of transcripts)
that separates treatment groups \cite{Clark:2014du}. 
The angle between each transcript's axis
and the hyperplane is used to define how much that gene contributes to the overall
differential expression between treatment groups.


\subsubsection{Fold change-based tests}

The MAQC project found that irreproducibility between
microarrays was largely due to the analyses, not due to the technology
or experimental variability \cite{Shi:2006dl}.
The most basic ranking statistic they tested, the fold change, 
was the most consistent between laboratories performing the same protocols.
Other statistics they used such as limma have the problem discussed in the previous
sections that many low-signal probes are ranked among the most significant DEGs.
It may be that many complex significance tests are biased to
the data sets on which they were tested. Several statistical
methods that are based on fold change, yet do not disregard
the variability in the data, are potential 
compromises (e.g. \cite{Dembele:2014cn,Yanofsky:2010by,Farztdinov:2012to,
Theilhaber:2001uc,McCarthy:2009dn,Stevens:2010jk,Deng:2009vh,Hein:2006jo}).


\subsection{What test should I use?}

Like with preprocessing, I recommend an exploratory approach
with significance testing, looking at the results from several tests. 
The different gene rankings from the same data
will put into perspective the confidence one should have
in any follow-up experiments. Since probes with low signals
are problematic (see previous sections), I also recommend reporting
expression values and fold changes along with any p-values
or statistical measures.


\section{Caveats of microarrays and alternatives}

As discussed, many of the problems with microarrays are low-intensity probes--
low-abundance transcripts. The number of statistical tests to correct
for this problem is exasperating, yet very few studies have
focused on experimental methods to improve the dynamic range
of arrays. 

Next generation sequencing of RNA (RNA-seq) is the
next step to improving variability. RNA-seq offer great
potential for reducing the uncertainty in transcript levels because
transcripts are counted. With microarrays, relative abundances
can only be estimated indirectly. However, RNA-seq presents
many new challenges as well. For instance, many of the sequenced
reads cannot be mapped to the genome. There also is not
a standard way to quantify expression
levels of entire genes based off of many different transcripts.

A major hindrance to any 
research with microarrays or sequencing is the availability
and reproducibility of analyses. I have taken
a special interest in reproducible research and will now discuss it briefly.

\section{Reproducibile analyses}

\subsection{Why bother?}

Irreproducible analyses are dangerous, perhaps bordering on unethical negligence. 
Dave et al. in the \textit{New England
Journal of Medicine} reported a marker for follicular lymphoma \cite{Dave:2004vl}. 
In letters to the editor, Tibshirani and Hong et al. stated they 
could not reproduce the analyses \cite{Tibshirani:2005el}. 
Dave et al. rectified the discrepancy as a misunderstanding
in how their data was interpreted. This is just one of a handful of
public disputes in the literature about gene expression analyses (two mentioned
in previous paragraphs).
Authors of disputed studies are most always well-intentioned, yet 
irreproducible analyses or poorly presented data raise suspicions.

In my opinion, analysts shouldn't fear being wrong; most researchers
are wrong most of the time. They must fear being overconfident or misleading. 
By making data and code open to criticism,
scientists protect their integrity and intellectual property in the same
way that lab notebooks do. They protect their colleagues
whose careers are dependent on their analyses as well as the patients
whose health decisions are affected by their research.
Finally, they increase there chance for mutually beneficial collaborations.

\subsection{Are microarrays reproducible?}

Thre has been great concern about inter-lab repeatability of 
gene expression experiments
(see example of poor reproducibility in 
\cite{Evsikov:2003ic,Fortunel:2003eu,Ivanova:2003bh,RamalhoSantos:2002dy}). 
Rather than discrediting microarray analysis,
one should also consider that microarrays
may reveal differences in \textit{seemingly} same experimental protocols.
For example, with the data presented in this dissertation, arrays from
replicate experiments on different days were clearly different when
visualized with principal components analysis. However, the differences
were systematic and could be corrected.

Problems may also arise from overexpectations and misunderstandings 
of what expression values can predict. 
For example, comparisons of microarray classification
studies (e.g., \cite{Veer:2002vv,Yeoh:2002wg,Alizadeh:2000tn,Golub:1999fn,
Perou:1999fr,Wang:2005uu,VanDeVijver:2002wj,Rosenwald:2002eg,Beer:2002uy,
Bhattacharjee:2001dt,Ramaswamy:2003to,Pomeroy:2002wx,Iizuka:2003wo})
with non-microarray studies have indicated predictive cancer markers or profiles
may not be as reliable as hoped \cite{Miklos:2004wn,Michiels:2007vs,Koscielny:2010gg,Eden:2004ua}.
Hundreds or even thousands of microarrays may be necessary to
accurately predict cancer 
outcomes \cite{Ioannidis:2005kd,Michiels:2005tg,EinDor:2006ga,Frantz:2005ur}.
However, a more rigorous re-evaluation, of a pessimistic study claiming
that microarray studies cannot predict cancer markers, found that
markers can indeed be found (see \cite{Michiels:2005tg,Fan:2010cv,Tong:2010ka,Koscielny:2010jd}
for the debate).
One shouldn't jump to conclusions from any one or even a few
analytical workflows.
For instance, two studies may find very different
list of differentially expressed genes, yet the correlation
between the data sets may be strong. Alternatively, two studies may
predict two different results that are both correct \cite{Zhang:2008bk,Zhang:2009cy}.

Several studies 
raise concerns about experimental reproducibility between
experimental platforms \cite{Tan:2003be,Kuo:2002cl,
Mah:2004ia,Rogojina:2003te,Woo:2004wz,Li:2002cz,Kothapalli:2002gz}.
Many irreproducibility claims were incorrect, first
dismissed by scientists at the FDA \cite{Shi:2005ik}.
The FDA and EPA coordinated a Microarray Quality Control (MAQC) project
to set standards and resolve outstanding questions 
\cite{Lesko:2004vi,Frueh:2006uy,Dix:2006ty,Shi:2004vh}.
With careful quality assurance and analyses,
microarray data were similar between 
laboratories \cite{Dobbin:2005wj,Irizarry:2005kb,Larkin:2005hb,
Ulrich:2004vw,Waring:2004vh,Weis:2005ut} and
platforms \cite{Petersen:2005by,Yauk:2004fq,
Park:2004ux,Yuen:2002ha,Canales:2006ts,Shippy:2006uf,
Patterson:2006hi,Tong:2006cg,Guo:2006wi}.


\section{Transcriptomics analyses in this dissertation}

In my dissertation, the first set of challenges have
been solving engineering problems---ensuring the reliability
and reproducibility of all analyses. This behind the scenes work is not highlighted
in all of the following chapters, yet it is central
to achieving the goals of each chapter.
In developing the best analytical strategies, I worked
through several previous publications. The greatest challenge
I and others have faced is reproducing these results.
I have made special efforts to make all computational
analyses repeatable. 

The next chapter describes a transcriptomics
analysis where that I have included to ensure reproducibility
of a clinical trial for which I analyzed the results.

As the hundreds of publications about microarray preprocessing
and significance tests have shown, engineers
and statisticians have been very interested in techniques and optimization.
However, the most difficult part of microarray studies comes
after the data processing, when biological interpretations need 
to be made. For example, new techniques may improve the accuracy
of DEG detection from 80\% to 90\%, but is that better accuracy
more helpful? Would a biologist find it useful that 9/10 of 
genes in a list are correct, not just 8/10? Maybe. Maybe not.

The following chapters focus on this transition from statistics
to biological understanding. In particular, transcriptomics analyses
of host cells to \textit{C. difficile} toxins are used to elucidate
changes in the replication of cells and proteins that contribute
or are markers for inflammation during disease.






